{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95acce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 原始权重： tensor([[[ 0.5926, -1.9402,  0.1706, -1.6533,  1.5114],\n",
      "         [-0.6901, -1.0532, -1.0856, -2.4421, -1.9638],\n",
      "         [-0.7968,  0.4638,  0.8605, -1.4075,  1.5474]],\n",
      "\n",
      "        [[-0.4394, -1.7094, -0.1538,  0.6890,  1.8248],\n",
      "         [ 0.3547, -0.5078, -2.1231, -0.2966,  0.0129],\n",
      "         [-0.4340, -0.7617, -1.0760, -0.8496,  1.3432]]])\n",
      " 归一化后的注意力权重： tensor([[[0.2300, 0.0183, 0.1508, 0.0243, 0.5765],\n",
      "         [0.3543, 0.2465, 0.2386, 0.0615, 0.0991],\n",
      "         [0.0482, 0.1701, 0.2529, 0.0262, 0.5026]],\n",
      "\n",
      "        [[0.0652, 0.0183, 0.0868, 0.2017, 0.6279],\n",
      "         [0.3653, 0.1542, 0.0307, 0.1904, 0.2595],\n",
      "         [0.1134, 0.0817, 0.0597, 0.0748, 0.6704]]])\n",
      "tensor([[[-0.3953,  0.8496,  0.1267,  0.3393],\n",
      "         [-0.3457,  0.2835,  0.3260, -0.2010],\n",
      "         [-0.4877,  0.4527,  0.4161,  0.3960]],\n",
      "\n",
      "        [[-0.0986,  0.4570,  0.0496, -1.2144],\n",
      "         [ 0.0711, -0.1518, -0.0274, -0.2183],\n",
      "         [ 0.1346,  0.6670,  0.1644, -1.1802]]])\n"
     ]
    }
   ],
   "source": [
    "import torch # 导入 torch\n",
    "import torch.nn.functional as F # 导入 nn.functional\n",
    "# 1. 创建两个张量 x1 和 x2\n",
    "x1 = torch.randn(2, 3, 4) # 形状 (batch_size, seq_len1, feature_dim)\n",
    "x2 = torch.randn(2, 5, 4) # 形状 (batch_size, seq_len2, feature_dim)\n",
    "# 2. 计算原始权重\n",
    "raw_weights = torch.bmm(x1, x2.transpose(1, 2)) # 形状 (batch_size, seq_len1, seq_len2)\n",
    "print(\" 原始权重：\", raw_weights) \n",
    "# 3. 用 softmax 函数对原始权重进行归一化\n",
    "attn_weights = F.softmax(raw_weights, dim=2) # 形状 (batch_size, seq_len1, seq_len2)\n",
    "print(\" 归一化后的注意力权重：\", attn_weights)\n",
    "# 4. 将注意力权重与 x2 相乘，计算加权和\n",
    "attn_output = torch.bmm(attn_weights, x2)  # 形状 (batch_size, seq_len1, feature_dim)\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建两个张量 x1 和 x2\n",
    "x1 = torch.randn(2, 3, 4) # 形状 (batch_size, seq_len1, feature_dim)\n",
    "x2 = torch.randn(2, 5, 4) # 形状 (batch_size, seq_len2, feature_dim)\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f661d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算点积，得到原始权重，形状为 (batch_size, seq_len1, seq_len2)\n",
    "raw_weights = torch.bmm(x1, x2.transpose(1, 2))\n",
    "print(\" 原始权重：\", raw_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe47f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # 导入 torch.nn.functional\n",
    "# 应用 softmax 函数，使权重的值在 0 和 1 之间，且每一行的和为 1\n",
    "attn_weights = F.softmax(raw_weights, dim=-1) # 归一化\n",
    "print(\" 归一化后的注意力权重：\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与 x2 相乘，得到注意力分布的加权和，形状为 (batch_size, seq_len1, feature_dim)\n",
    "attn_output = torch.bmm(attn_weights, x2)\n",
    "print(\" 注意力输出 :\", attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84776b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
