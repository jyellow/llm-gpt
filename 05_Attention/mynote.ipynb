{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 定义参数\n",
    "VOCAB_SIZE = 1000\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            sentence = torch.randint(0, VOCAB_SIZE, (20,))  # 20个词的句子\n",
    "            label = torch.randint(0, NUM_CLASSES, (1,))\n",
    "            self.data.append((sentence, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = SimpleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (EMBED_DIM ** 0.5)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.mean(dim=1)  # 对所有词向量取平均\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 43.9184, Accuracy: 50.20%\n",
      "Epoch [2/10], Loss: 43.5044, Accuracy: 53.70%\n",
      "Epoch [3/10], Loss: 43.1205, Accuracy: 55.40%\n",
      "Epoch [4/10], Loss: 42.6211, Accuracy: 59.10%\n",
      "Epoch [5/10], Loss: 41.4798, Accuracy: 62.90%\n",
      "Epoch [6/10], Loss: 39.8191, Accuracy: 65.90%\n",
      "Epoch [7/10], Loss: 37.3090, Accuracy: 69.40%\n",
      "Epoch [8/10], Loss: 34.3026, Accuracy: 73.30%\n",
      "Epoch [9/10], Loss: 30.6382, Accuracy: 77.60%\n",
      "Epoch [10/10], Loss: 26.6460, Accuracy: 80.90%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# 创建模型并开始训练\n",
    "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES)\n",
    "train_model(model, dataloader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 将文本转换为模型可以处理的格式\n",
    "    # 这里假设我们有一个函数可以将文本转换为词索引\n",
    "    input_tensor = text_to_tensor(text).unsqueeze(0)  # 添加批次维度\n",
    "    \n",
    "    # 进行预测\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    # 获取预测结果\n",
    "    confidence, predicted = torch.max(output, 1)\n",
    "        # 获取具体的预测值\n",
    "    predicted_value = predicted.item()\n",
    "    confidence_value = confidence.item()\n",
    "    all_probs = probabilities.squeeze().tolist()\n",
    "    labels = [\"负面\", \"正面\"]\n",
    "    predicted_label = labels[predicted_value]\n",
    "    print(probabilities)\n",
    "    print(confidence)\n",
    "    print(predicted)\n",
    "    print(predicted_value)\n",
    "    print(confidence_value)\n",
    "    print(all_probs)\n",
    "    print(predicted_label)\n",
    "    # 将预测结果转换为标签\n",
    "    label = \"正面\" if predicted.item() == 1 else \"负面\"\n",
    "    \n",
    "    return label\n",
    "\n",
    "# 假设的文本到张量转换函数\n",
    "def text_to_tensor(text):\n",
    "    # 这里应该实现将文本转换为词索引的逻辑\n",
    "    # 为了简单起见,我们这里只是随机生成一个张量\n",
    "    return torch.randint(0, VOCAB_SIZE, (20,))\n",
    "\n",
    "# 使用示例\n",
    "# model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES)\n",
    "# 假设模型已经训练好了\n",
    "# model.load_state_dict(torch.load('trained_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2933, 0.7067]])\n",
      "tensor([0.5580])\n",
      "tensor([1])\n",
      "1\n",
      "0.5579589009284973\n",
      "[0.29331257939338684, 0.7066874504089355]\n",
      "正面\n",
      "评论 'I hate you.' 的情感预测结果是: 正面\n"
     ]
    }
   ],
   "source": [
    "# 进行预测\n",
    "sample_text = \"I hate you.\"\n",
    "prediction = predict(model, sample_text)\n",
    "print(f\"评论 '{sample_text}' 的情感预测结果是: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
